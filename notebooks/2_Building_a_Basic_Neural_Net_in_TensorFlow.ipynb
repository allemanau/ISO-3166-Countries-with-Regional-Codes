{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2: Building a Basic Neural Net in TensorFlow",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNna6jbS1TwOKMguWJ0fx2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allemanau/ISO-3166-Countries-with-Regional-Codes/blob/master/notebooks/2_Building_a_Basic_Neural_Net_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dJlEEGhE5_t",
        "colab_type": "text"
      },
      "source": [
        "# __2: Building a Basic Neural Net in TensorFlow__\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "In this notebook, we'll learn...\n",
        "- the basic components of a deep neural network\n",
        "- how to express and combine them into a model in TensorFlow\n",
        "- how to visualize a declared model\n",
        "- how to compile and fit a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6or-5BxPYHG",
        "colab_type": "text"
      },
      "source": [
        "## __The Sequential Model__\n",
        "\n",
        "The vast majority of deep learning applications employ a [sequential structure](https://keras.io/getting-started/sequential-model-guide/), meaning that layers are stacked in order from the input to output layers. In the example below, the input layer passes to hidden layer 1, layer 1 to hidden layer 2, and finally layer 2 to the output layer.\n",
        "\n",
        "<img src=\"https://github.com/allemanau/NUIT_tensorflow/blob/master/images/sequential_model.png?raw=1\">\n",
        "\n",
        "We build sequential models progressively by adding layers to a model object. To start adding layers, though, we need an empty model to start. Using Keras, a sequential model can be declared as follows:\n",
        "```\n",
        "model = Sequential()\n",
        "```\n",
        "Let's load up TensorFlow and actually declare an empty model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z24-ocMnSmqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import latest TensorFlow 2 release\n",
        "# Until 2.x is the default in Colab, stipulates that we want to load the latest version of TensorFlow 2.0, not 1.x.\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Reshape, Flatten\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "\n",
        "# For simulating data and plotting results.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSJzsphSzII",
        "colab_type": "text"
      },
      "source": [
        "Since we've expressly imported the `tensorflow.keras.models` module, we instantiate the model as below. We can visualize the structure of model using a utility from `tensorflow.keras.utils`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNZd3_4LS0yM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = Sequential()\n",
        "\n",
        "# We can use the plot_model function from tensorflow.keras.utils to visualize our empty model\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdFJlrjLX-Dl",
        "colab_type": "text"
      },
      "source": [
        "### Adding layers to a model\n",
        "\n",
        "Once a model is instantiated, you can add a layer of neurons to it using the `add()` method:\n",
        "```\n",
        "my_model = Sequential()\n",
        "my_model.add(...)\n",
        "```\n",
        "But before we actually try this, we should discuss the types of layers available to add!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU4viTpSJF07",
        "colab_type": "text"
      },
      "source": [
        "## __Core Layers__\n",
        "\n",
        "TensorFlow offers a collection of so-called [core layers](https://keras.io/layers/core/). These constructs are the building blocks of most neural networks. Let's discuss the ones that will get us up and running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EuNhO7cTj4W1"
      },
      "source": [
        "### Input layer\n",
        "\n",
        "Input layers __explicitly__ specify the input shape (excluding number of examples) of the input. For example, if our inputs were vectors of length 3, we could do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-44pmGdUkgnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = Sequential()\n",
        "my_model.add(Input(shape = (3,)))\n",
        "\n",
        "# We can use the plot_model function from tensorflow.keras.utils to visualize our empty model.\n",
        "# show_shapes = True ensures the input and output dimensions for each layer are reported.\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugkjc_VKkhsJ",
        "colab_type": "text"
      },
      "source": [
        "Now we have the very first layer of a model. The '?' in the `plot_model` output indicates that the number of examples varies. Each example is a 1D tensor of size 3. Thus, if we have `n` examples, our input data is a 2D tensor of size `(n, 3)`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqNFr-XPLYaS",
        "colab_type": "text"
      },
      "source": [
        "### Dense layer\n",
        "\n",
        "`Dense` layers are the hidden layers you might be familiar with in standard neural networks. They are fully connected to inputs in the preceding layer, and outputs in the following layer. This small example is composed only of `Dense` layers:\n",
        "\n",
        "<img src=\"https://github.com/allemanau/NUIT_tensorflow/blob/master/images/larger_example_network.png?raw=1\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI-1PMKskA-G",
        "colab_type": "text"
      },
      "source": [
        "__Hidden layer 1__ is a `Dense` layer with 4 units. We could add it to a model with an existing `Input` layer like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN1DNoFdk6Oo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From before, just to reiterate.\n",
        "my_model = Sequential()\n",
        "my_model.add(Input(shape = (3,)))\n",
        "\n",
        "# Add hidden layer 1\n",
        "my_model.add(Dense(units = 4))\n",
        "\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kj5DMzzAj6t7"
      },
      "source": [
        "If we haven't specified `Input` we can use the `input_shape` argument to implicitly define it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6tr0hsVlPAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Redeclare empty my_model. Notice no input layer is explicitly specified.\n",
        "my_model = Sequential()\n",
        "my_model.add(Dense(units = 4, input_shape = (3,)))\n",
        "\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ4AS2zwkqm5",
        "colab_type": "text"
      },
      "source": [
        "In a sequential model in TensorFlow, __hidden layer 2__ is aware of hidden layer 1's shape, so assuming we've already added hidden layer 1 we just declare hidden layer 2 as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bILpkJBWm-ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model.add(Dense(4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n5IehwkmVNC",
        "colab_type": "text"
      },
      "source": [
        "In general classification problems, the output layer will be a `Dense` layer of the same size as the number of possible classes. (__Note:__ binary problems have an equivalent representation with one neuron in the output layer. For consistency, we'll ignore this case.) In this layer, the node with the highest activation value for a given input determines the class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc38DxKjm00s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model.add(Dense(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxKfcJESnDt2",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the model diagram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UCmFZJinHWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjY1kuQTOJOG",
        "colab_type": "text"
      },
      "source": [
        "### Activation\n",
        "\n",
        "The choice of activation function at each layer determines network performance and convergence of the weights in the training process. In the previous exercise, notice no mention was made of activation functions; in this case, a linear activation function is assumed. To change the activation function for a layer, we can sandwich in `Activation` layers, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmTlm1rYlmQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Redeclare empty my_model.\n",
        "my_model = Sequential()\n",
        "my_model.add(Dense(units = 4, input_shape = (3,)))\n",
        "my_model.add(Activation(\"relu\"))\n",
        "\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCbXkcGsn5pS",
        "colab_type": "text"
      },
      "source": [
        "A more concise way to layer in activation is just to use the `activation` argument in the `Dense()` call, as below. But be careful -- notice the visualization does not display an `Activation` layer in this case, so be careful in assembling your model! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFQ_l4PFoprd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Redeclare empty my_model.\n",
        "my_model = Sequential()\n",
        "my_model.add(Dense(units = 4, input_shape = (3,), activation = \"relu\"))\n",
        "\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M3CkHcapB-a",
        "colab_type": "text"
      },
      "source": [
        "As with all the various components of deep learning, there's an art and a science to choosing activation functions. A full list of available activations can be found in the [documentation](https://keras.io/activations/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K0KBpm5cBty",
        "colab_type": "text"
      },
      "source": [
        "### Combining Dense and Activation Layers\n",
        "\n",
        "Let's build a quick model. Here's a network diagram for binary classification. Assume the hidden layer is activated by a sigmoid, and the output layer a softmax.\n",
        "\n",
        "<img src=\"https://github.com/allemanau/NUIT_tensorflow/blob/master/images/exercise_1.png?raw=1\">\n",
        "\n",
        "In TensorFlow, this model could be built as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve06ud8kcSdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare empty model.\n",
        "my_model = Sequential()\n",
        "\n",
        "# Add the input layer.\n",
        "my_model.add(Input(shape = (2,)))\n",
        "\n",
        "# Add a dense layer after the input layer.\n",
        "my_model.add(Dense(units = 3))\n",
        "\n",
        "# Activate the previous layer using sigmoids.\n",
        "my_model.add(Activation(activation = \"sigmoid\"))\n",
        "\n",
        "# Add an output layer. Our output layer has two classes, and hence is of size 2.\n",
        "my_model.add(Dense(units = 2))\n",
        "\n",
        "# Activate the output layer using softmax.\n",
        "my_model.add(Activation(activation = \"softmax\"))\n",
        "\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQPEwIQJUeAm",
        "colab_type": "text"
      },
      "source": [
        "### __Exercise 1: building a sequential model (~10 min)__\n",
        "\n",
        "Build a version of the network we've been referencing as an example: \n",
        "\n",
        "<img src=\"https://github.com/allemanau/NUIT_tensorflow/blob/master/images/larger_example_network.png?raw=1\">\n",
        "\n",
        "The hidden layers should be sigmoid (\"sigmoid\") activated, and the last layer should be softmax (\"softmax\") activated. First, do it with `Activation` layer calls. Then, do it with the `activation` argument in `Dense()` calls (`Dense(..., activation = \"blah\")`). In both cases, use `plot_model` to visualize.\n",
        "\n",
        "### Ex 1a: using `Dense` and `Activation` layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHb4zU-_bUUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare empty model.\n",
        "ex1a = Sequential()\n",
        "\n",
        "### Add layers here.\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "plot_model(ex1a, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7G4Vcdwkj3-S"
      },
      "source": [
        "### Ex 1b: using `activation = ` arguments in `Dense()` calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oik6SYKbj3oO",
        "colab": {}
      },
      "source": [
        "# Declare empty model.\n",
        "ex1b = Sequential()\n",
        "\n",
        "### Add layers here.\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "plot_model(ex1b, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2ExNF4oLVoP",
        "colab_type": "text"
      },
      "source": [
        "### Dropout Layers\n",
        "\n",
        "Dropout is one efficient way to avoid overfitting as part of the training process. Dropout randomly deactivates a fraction of the nodes in a layer during each iteration of the training process. If you'd like to know how dropout helps to prevent overfitting, see this [gentle but detailed introduction](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/).\n",
        "\n",
        "![](https://miro.medium.com/proxy/1*iWQzxhVlvadk6VAJjsgXgg.png)\n",
        "\n",
        "Like `Activation`, `Dropout` applies to the preceding layer. In actuality, dropout doesn't make a lot of sense for this particular network, but for larger, more complex architectures, we could use it to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg3uRzhNxiIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = Sequential()\n",
        "my_model.add(Input(shape = (2,)))\n",
        "my_model.add(Dense(units = 3))\n",
        "my_model.add(Activation(activation = \"sigmoid\"))\n",
        "\n",
        "# Add a dropout layer\n",
        "my_model.add(Dropout(rate = 0.33))\n",
        "\n",
        "my_model.add(Dense(units = 2))\n",
        "my_model.add(Activation(activation = \"softmax\"))\n",
        "\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV-vv8LhyxIY",
        "colab_type": "text"
      },
      "source": [
        "### Flatten and Reshape\n",
        "\n",
        "\n",
        "`Reshape` converts a tensor of given dimension into one of another. In order for `Reshape` to work, the dimensions have to agree.\n",
        "\n",
        "`Flatten` simply converts a layer with more than one dimension to a 1D tensor. Useful for transforming input or hidden layers of dimension greater than one. If there is a special ordering in which the data should be flattened, the `data_format` argument can be used (although we won't use it).\n",
        "\n",
        "Here's an example using a `Flatten` layer to undo a `Reshape` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBkpcZV9zuZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = Sequential()\n",
        "my_model.add(Input(shape = (8,)))\n",
        "\n",
        "# Here, reshape converts 1D length-10 tensor into a 3D 2x2x2 tensor:\n",
        "my_model.add(Reshape(target_shape = (2,2,2)))\n",
        "\n",
        "# Flatten undoes the works Reshape just did:\n",
        "my_model.add(Flatten())\n",
        "\n",
        "plot_model(my_model, show_shapes = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGmhwLUP07sQ",
        "colab_type": "text"
      },
      "source": [
        "## __Compiling a Model__\n",
        "\n",
        "In order to train a model and use it to make predictions, the model needs to be \"compiled\", or configured for training. In this phase, we also specify some important components of the model such as [__loss function(s)__](https://keras.io/losses/) to minimize, [__metric(s)__](https://keras.io/metrics/) to report, and an [__optimizer__](https://keras.io/optimizers/) to use, among other options. This is a simple step if you know how to specify these options:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIRSXfOM2M4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_model = Sequential()\n",
        "my_model.add(Input(shape = (3,)))\n",
        "my_model.add(Dense(units = 4))\n",
        "my_model.add(Activation(activation = \"sigmoid\"))\n",
        "my_model.add(Dense(units = 4))\n",
        "my_model.add(Activation(activation = \"sigmoid\"))\n",
        "my_model.add(Dense(units = 2))\n",
        "my_model.add(Activation(activation = \"softmax\"))\n",
        "\n",
        "# Train the network for binary classification using the following options:\n",
        "# Optimization routine = Stochastic Gradient Descent ('sgd')\n",
        "# Loss = binary cross-entropy (for multi-class problems, use 'categorical_crossentropy')\n",
        "# Metric = classification accuracy. Can be a list of multiple metrics if desired.\n",
        "my_model.compile(optimizer = 'adam',\n",
        "                 loss = 'binary_crossentropy',\n",
        "                 metrics = ['accuracy'])\n",
        "\n",
        "# No errors means our model compiled fine."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTldkO9i4wSS",
        "colab_type": "text"
      },
      "source": [
        "Aside from the fact that you have to make sure the options make sense for your problem, the only trick here is that any time you change your model architecture or options, __it needs to be rebuilt and recompiled before training.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv8yT0Cp4rv2",
        "colab_type": "text"
      },
      "source": [
        "## __Training and Evaluating a Compiled Network__\n",
        "\n",
        "Only one step left before we have a fully operational Death Star -- err, model -- the __training phase__. Like model compilation, this is a straightforward step if you know some specifics:\n",
        "\n",
        "- `epochs`: how long should the model be trained for? \n",
        "- `batch_size`: how many examples should the model analyze and update on at a time?\n",
        "\n",
        "Let's start by simulating a binary classification problem for a network we've already built:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdgQJ3mv4Boj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set random initial seed, so we all have the same data.\n",
        "np.random.seed(2020)\n",
        "\n",
        "# Generate 1000 3-D points distributed in [0,1]x[0,1]x[0,1]\n",
        "x_train = np.random.random((1000, 3))\n",
        "\n",
        "# Generate nonlinear labels correlated with the random values in data.\n",
        "def label_gen(x):\n",
        "  return (x[0]*x[1] + np.random.normal(scale = .1)) > .5\n",
        "\n",
        "# Run the label_gen function for each row of x_train.\n",
        "labels = np.apply_along_axis(label_gen, 1, x_train)\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "y_train = to_categorical(labels, num_classes=2)\n",
        "\n",
        "print(\"Training data:\")\n",
        "print(x_train)\n",
        "print(\"\\nOne-hot encoded training labels:\")\n",
        "print(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiqLlfz786gv",
        "colab_type": "text"
      },
      "source": [
        "Now, training the model is as simple as feeding the training and test data to the `fit()` method. We'll let it go for 500 epochs and see what happens.\n",
        "\n",
        "__Note 1:__ `my_model` itself is permanently modified by the training process.\n",
        "\n",
        "__Note 2:__ although it may sound strange given how the code looks, `history` is not the trained model! It is a history of the training process, and offers many useful analysis and visualization tools. We'll see one in a second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaTqzI3O4Fgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = my_model.fit(x = x_train, \n",
        "                       y = y_train,\n",
        "                       epochs = 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjMfZW8tDzAr",
        "colab_type": "text"
      },
      "source": [
        "The `history` object enables us to plot improvement in training accuracy and loss over our training epochs. If everything's working correctly, we expect to see accuracy increase and loss decrease over time. If not, it's time to do a little detective work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2JWz1_hBNr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMbZGu_0EskI",
        "colab_type": "text"
      },
      "source": [
        "Of course, the purpose of our model is make accurate predictions on data it hasn't yet seen. Let's generate some random test data the same way we did training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABYACqktBitj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2021)\n",
        "\n",
        "# Generate 1000 3-D points distributed in [0,1]x[0,1]x[0,1]\n",
        "x_test = np.random.random((1000, 3))\n",
        "\n",
        "# Generate nonlinear labels correlated with the random values in data.\n",
        "def label_gen(x):\n",
        "  return (x[0]*x[1] + np.random.normal(scale = .1)) > .5\n",
        "\n",
        "# Run the label_gen function for each row of x_test.\n",
        "labels = np.apply_along_axis(label_gen, 1, x_test)\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "y_test = to_categorical(labels, num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt4ntjezFG4F",
        "colab_type": "text"
      },
      "source": [
        "Now, using the `evaluate()` method, we can determine how well our model does out-of-sample. No seed's been set for the random test data, so everyone will have different results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olW7mGpiBtEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = my_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\")\n",
        "print(\"{}%\".format(np.round(100*score[1], 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxcTrLETaXhc",
        "colab_type": "text"
      },
      "source": [
        "Since the test and training data have been generated by the same process, we expect similar accuracy and loss on the test data, although we've done nothing to ensure our model hasn't overfitted the training data (more on this in a bit). Glancing at the training accuracy plot may give you a sense of when it makes sense to stop training the model for efficiency's sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD4PNYeJardb",
        "colab_type": "text"
      },
      "source": [
        "### __Exercise 2: modifying network parameters (~5 min)__\n",
        "\n",
        "Without too much time and effort, you can modify the parameters and network architecture to get $\\ge93\\%$ test accuracy virtually every time you train it.\n",
        "\n",
        "__Try a different activation function in the hidden layers__. For instance, does it make a difference if you use ReLU instead of sigmoid? What if you mix activation functions?\n",
        "\n",
        "__Play around with the hidden layers.__ Do we need both layers? Remove one and see what happens. Does adding more units matter?\n",
        "\n",
        "__Choose a suitable number of epochs to train the model for, based on the prior accuracy and loss plots__. Compare your test accuracy with the accuracy you obtained when training for 500 epochs. (Keep in mind that the training process itself is subject to randomness, so your results may vary fit to fit even if the number of epochs remains the same.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqlC7-Eia6b1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "my_model = Sequential()\n",
        "my_model.add(Input(shape = (3,)))\n",
        "my_model.add(Dense(units = 4))\n",
        "my_model.add(Activation(activation = \"sigmoid\"))\n",
        "my_model.add(Dense(units = 4))\n",
        "my_model.add(Activation(activation = \"sigmoid\"))\n",
        "my_model.add(Dense(units = 2))\n",
        "my_model.add(Activation(activation = \"softmax\"))\n",
        "\n",
        "my_model.compile(optimizer = 'adam',\n",
        "                 loss = 'binary_crossentropy',\n",
        "                 metrics = ['accuracy'])\n",
        "\n",
        "history = my_model.fit(x = x_train, \n",
        "                       y = y_train,\n",
        "                       epochs = num_epochs,\n",
        "                       verbose = 0)\n",
        "\n",
        "score = my_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\")\n",
        "print(\"{}%\".format(np.round(100*score[1], 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVEEERWsJuf8",
        "colab_type": "text"
      },
      "source": [
        "## __Review__\n",
        "\n",
        "Concepts to know:\n",
        "\n",
        "- __The sequential model__\n",
        "  - What does it mean for layers to be sequentially organized?\n",
        "- __Layers__. For each of `Input`, `Dense`, `Activation`, and `Dropout`:\n",
        "  - How are they structured? \n",
        "  - What does each one do?\n",
        "  - What are their input and output sizes? \n",
        "- What do `Flatten` and `Reshape` do?\n",
        "- __Compilation__\n",
        "  - What is the purpose of compilation? \n",
        "  - What important options can you adjust at compilation time? \n",
        "  - Do you ever need to recompile your model?\n",
        "- __Training__\n",
        "  - What does training do to a neural network?\n",
        "  - What is a `history` object useful for?\n",
        "  - What is an epoch?\n",
        "- __Evaluation__\n",
        "  - What is the purpose of evaluation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdSiM2Fo2B6v",
        "colab_type": "text"
      },
      "source": [
        "## __Next__: building a neural network on real data"
      ]
    }
  ]
}